{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed86965",
   "metadata": {},
   "source": [
    "## Appendix — Baselines & Experiments\n",
    "\n",
    "Nesta seção estão modelos iniciais testados apenas para referência histórica:\n",
    "\n",
    "- Logistic Regression (AUC ~0.74, convergência lenta).\n",
    "- ElasticNet (AUC ~0.73, muito pesado, não adotado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "num_cols  = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "cat_cols  = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "low_card  = [c for c in cat_cols if X[c].nunique() < 10]\n",
    "high_card = [c for c in cat_cols if X[c].nunique() >= 10]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\",  RobustScaler())])\n",
    "\n",
    "cat_low_pipe  = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "cat_high_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"tgt\", TargetEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\",  num_pipe,      num_cols),\n",
    "    (\"ohe\",  cat_low_pipe,  low_card),\n",
    "    (\"tgt\",  cat_high_pipe, high_card),\n",
    "])\n",
    "\n",
    "# Model\n",
    "clf = LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", max_iter=2000, class_weight=\"balanced\")\n",
    "\n",
    "pipe = Pipeline([(\"prep\", preprocessor),\n",
    "                 (\"clf\",  clf)])\n",
    "\n",
    "# Helpers\n",
    "def ks_statistic(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return np.max(tpr - fpr)\n",
    "\n",
    "def precision_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "def recall_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "# CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(X))\n",
    "fold_rows = []\n",
    "\n",
    "for i, (tr, va) in enumerate(cv.split(X, y), 1):\n",
    "    X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    proba = pipe.predict_proba(X_va)[:, 1]\n",
    "    oof[va] = proba\n",
    "\n",
    "    auc = roc_auc_score(y_va, proba)\n",
    "    ks  = ks_statistic(y_va, proba)\n",
    "    p20 = precision_at_k(y_va, proba, k=0.2)\n",
    "    r20 = recall_at_k(y_va, proba, k=0.2)\n",
    "    fold_rows.append({\"fold\": i, \"AUC\": auc, \"KS\": ks, \"P@20\": p20, \"R@20\": r20})\n",
    "    print(f\"[Fold {i}] AUC={auc:.4f} | KS={ks:.4f} | P@20={p20:.4f} | R@20={r20:.4f}\")\n",
    "\n",
    "df_folds = pd.DataFrame(fold_rows)\n",
    "auc_all = roc_auc_score(y, oof)\n",
    "ks_all  = ks_statistic(y, oof)\n",
    "p20_all = precision_at_k(y, oof, k=0.2)\n",
    "r20_all = recall_at_k(y, oof, k=0.2)\n",
    "\n",
    "print(\"\\n[OOF] AUC={:.4f} | KS={:.4f} | P@20={:.4f} | R@20={:.4f}\".format(auc_all, ks_all, p20_all, r20_all))\n",
    "display(df_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b382a77",
   "metadata": {},
   "source": [
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 1] AUC=0.7389 | KS=0.3545 | P@20=0.1996 | R@20=0.4945\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 2] AUC=0.7504 | KS=0.3713 | P@20=0.2006 | R@20=0.4971\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 3] AUC=0.7445 | KS=0.3640 | P@20=0.2005 | R@20=0.4967\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 4] AUC=0.7488 | KS=0.3737 | P@20=0.2053 | R@20=0.5086\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 5] AUC=0.7379 | KS=0.3539 | P@20=0.1960 | R@20=0.4856\n",
    "\n",
    "[OOF] AUC=0.7441 | KS=0.3613 | P@20=0.2003 | R@20=0.4962"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c359293",
   "metadata": {},
   "source": [
    "\n",
    "fold\tAUC\tKS\tP@20\tR@20\n",
    "0\t1\t0.738865\t0.354508\t0.199577\t0.494461\n",
    "1\t2\t0.750388\t0.371348\t0.200634\t0.497080\n",
    "2\t3\t0.744543\t0.363961\t0.200472\t0.496677\n",
    "3\t4\t0.748788\t0.373713\t0.205268\t0.508560\n",
    "4\t5\t0.737892\t0.353851\t0.196000\t0.485599\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b14e3b",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression — Results & Takeaways\n",
    "\n",
    "My 5-fold OOF results are:\n",
    "\n",
    "- **AUC:** 0.7441  \n",
    "- **KS:** 0.3613  \n",
    "- **Precision@20%:** 0.2003  \n",
    "- **Recall@20%:** 0.4962  \n",
    "\n",
    "This is a solid baseline. The lift at the top 20% is meaningful, and KS is in an acceptable range for a first pass.\n",
    "\n",
    "I did observe a **ConvergenceWarning** from `lbfgs` (max_iter hit the limit). This typically happens with high-dimensional feature spaces (OHE + target encoders). I’ll fix it by increasing `max_iter` and slightly strengthening regularization (**lower C**). That keeps the solution stable and avoids overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86bab0",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression — Results & Takeaways\n",
    "\n",
    "My 5-fold OOF results are:\n",
    "\n",
    "- **AUC:** 0.7441  \n",
    "- **KS:** 0.3613  \n",
    "- **Precision@20%:** 0.2003  \n",
    "- **Recall@20%:** 0.4962  \n",
    "\n",
    "This is a solid baseline. The lift at the top 20% is meaningful, and KS is in an acceptable range for a first pass.\n",
    "\n",
    "I did observe a **ConvergenceWarning** from `lbfgs` (max_iter hit the limit). This typically happens with high-dimensional feature spaces (OHE + target encoders). I’ll fix it by increasing `max_iter` and slightly strengthening regularization (**lower C**). That keeps the solution stable and avoids overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\",\n",
    "    C=0.5,         \n",
    "    max_iter=5000,  \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\",  clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e232d61",
   "metadata": {},
   "source": [
    "# Stratified 5-Fold CV — OOF metrics\n",
    "\n",
    "I evaluate the baseline with Stratified K-Fold CV (no leakage).  \n",
    "Metrics reported: **ROC-AUC**, **KS**, **Precision@20%**, **Recall@20%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc023991",
   "metadata": {},
   "source": [
    "def ks_statistic(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return np.max(tpr - fpr)\n",
    "\n",
    "def precision_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "def recall_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "rows = []\n",
    "\n",
    "for i, (tr, va) in enumerate(cv.split(X, y), 1):\n",
    "    X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    proba = pipe.predict_proba(X_va)[:, 1]\n",
    "    oof[va] = proba\n",
    "\n",
    "    auc = roc_auc_score(y_va, proba)\n",
    "    ks  = ks_statistic(y_va, proba)\n",
    "    p20 = precision_at_k(y_va, proba, k=0.2)\n",
    "    r20 = recall_at_k(y_va, proba, k=0.2)\n",
    "    rows.append({\"fold\": i, \"AUC\": auc, \"KS\": ks, \"P@20\": p20, \"R@20\": r20})\n",
    "    print(f\"[Fold {i}] AUC={auc:.4f} | KS={ks:.4f} | P@20={p20:.4f} | R@20={r20:.4f}\")\n",
    "\n",
    "df_folds = pd.DataFrame(rows)\n",
    "auc_all = roc_auc_score(y, oof)\n",
    "ks_all  = ks_statistic(y, oof)\n",
    "p20_all = precision_at_k(y, oof, k=0.2)\n",
    "r20_all = recall_at_k(y, oof, k=0.2)\n",
    "\n",
    "print(\"\\n[OOF] AUC={:.4f} | KS={:.4f} | P@20={:.4f} | R@20={:.4f}\".format(auc_all, ks_all, p20_all, r20_all))\n",
    "display(df_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f3492",
   "metadata": {},
   "source": [
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 1] AUC=0.7407 | KS=0.3603 | P@20=0.2007 | R@20=0.4973\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 2] AUC=0.7525 | KS=0.3746 | P@20=0.2027 | R@20=0.5023\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 3] AUC=0.7460 | KS=0.3645 | P@20=0.2021 | R@20=0.5007\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 4] AUC=0.7505 | KS=0.3775 | P@20=0.2057 | R@20=0.5096\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "[Fold 5] AUC=0.7401 | KS=0.3569 | P@20=0.1971 | R@20=0.4884\n",
    "\n",
    "[OOF] AUC=0.7459 | KS=0.3645 | P@20=0.2018 | R@20=0.5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d783610",
   "metadata": {},
   "source": [
    "\n",
    "fold\tAUC\tKS\tP@20\tR@20\n",
    "0\t1\t0.740652\t0.360300\t0.200715\t0.497281\n",
    "1\t2\t0.752507\t0.374647\t0.202748\t0.502316\n",
    "2\t3\t0.745985\t0.364495\t0.202097\t0.500705\n",
    "3\t4\t0.750539\t0.377460\t0.205674\t0.509567\n",
    "4\t5\t0.740120\t0.356866\t0.197138\t0.488419\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb89d85",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression — OOF Results\n",
    "\n",
    "My 5-fold OOF metrics with the stability patch are:\n",
    "\n",
    "- **AUC:** 0.7459  \n",
    "- **KS:** 0.3645  \n",
    "- **Precision@20%:** 0.2018  \n",
    "- **Recall@20%:** 0.5000  \n",
    "\n",
    "This is a solid starting point. I still see a `ConvergenceWarning` from `lbfgs` (even at `max_iter=5000`), which is common in high-dimensional spaces after OHE/encoders. To keep the baseline fully stable (and sometimes a tad better), I’ll switch to the `saga` solver (supports large feature spaces and elastic-net regularization). I’ll keep L2 first; if needed, I’ll try a small L1 component (`elasticnet`) to shrink noisy coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6fac0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "788d76f8",
   "metadata": {},
   "source": [
    "# Elastic Net variant for high-dimensional OHE\n",
    "\n",
    "As an alternative, I can try `solver=\"saga\"` with `elasticnet`, which often behaves well in wide feature spaces by shrinking coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faa72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Logistic Regression (saga + elasticnet) — optimized CV block ===\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from joblib import Memory\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Assumes X, y, and `preprocessor` are already defined above ---\n",
    "# If not, ensure you run the preprocessing cell first.\n",
    "\n",
    "# Cache (helps when re-running the same pipeline code during experimentation)\n",
    "cache = Memory(location=\"artifacts/sk_cache\", verbose=0)\n",
    "\n",
    "# Metrics helpers (idempotent: safe to re-run)\n",
    "def ks_statistic(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "def precision_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return float(precision_score(y_true, y_pred))\n",
    "\n",
    "def recall_at_k(y_true, y_score, k=0.2):\n",
    "    n = len(y_true); topk = int(np.ceil(k*n))\n",
    "    idx = np.argsort(-y_score)[:topk]\n",
    "    y_pred = np.zeros(n, dtype=int); y_pred[idx] = 1\n",
    "    return float(recall_score(y_true, y_pred))\n",
    "\n",
    "# CV config (set to 3 for quick iteration; 5 for final)\n",
    "N_SPLITS = 5\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Model: saga + elasticnet (faster/more stable settings) ---\n",
    "# Tip: if training is still slow, switch to penalty='l2' (same solver) and/or N_SPLITS=3\n",
    "clf_en = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.15,      # smaller L1 -> faster convergence than 0.2\n",
    "    C=0.6,              # a bit stronger regularization (speeds up, reduces overfit)\n",
    "    max_iter=3000,      # 3000 + tol=2e-3 converge bem mais rápido que 5000 + tol=1e-4\n",
    "    tol=2e-3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe_en = Pipeline(\n",
    "    steps=[(\"prep\", preprocessor), (\"clf\", clf_en)],\n",
    "    memory=cache\n",
    ")\n",
    "\n",
    "# --- OOF evaluation ---\n",
    "oof_en = np.zeros(len(X), dtype=float)\n",
    "rows_en = []\n",
    "\n",
    "t0_all = time.time()\n",
    "for i, (tr, va) in enumerate(cv.split(X, y), 1):\n",
    "    t0 = time.time()\n",
    "    X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    pipe_en.fit(X_tr, y_tr)\n",
    "    proba = pipe_en.predict_proba(X_va)[:, 1]\n",
    "    oof_en[va] = proba\n",
    "\n",
    "    auc = roc_auc_score(y_va, proba)\n",
    "    ks  = ks_statistic(y_va, proba)\n",
    "    p20 = precision_at_k(y_va, proba, k=0.2)\n",
    "    r20 = recall_at_k(y_va, proba, k=0.2)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    rows_en.append({\"fold\": i, \"AUC\": auc, \"KS\": ks, \"P@20\": p20, \"R@20\": r20, \"time_s\": dt})\n",
    "    print(f\"[EN Fold {i}] AUC={auc:.4f} | KS={ks:.4f} | P@20={p20:.4f} | R@20={r20:.4f} | {dt:.1f}s\")\n",
    "\n",
    "# Overall OOF\n",
    "auc_all_en = roc_auc_score(y, oof_en)\n",
    "ks_all_en  = ks_statistic(y, oof_en)\n",
    "p20_all_en = precision_at_k(y, oof_en, k=0.2)\n",
    "r20_all_en = recall_at_k(y, oof_en, k=0.2)\n",
    "dt_all = time.time() - t0_all\n",
    "\n",
    "df_folds_en = pd.DataFrame(rows_en)\n",
    "print(\"\\n[EN OOF] AUC={:.4f} | KS={:.4f} | P@20={:.4f} | R@20={:.4f} | total={:.1f}s\"\n",
    "      .format(auc_all_en, ks_all_en, p20_all_en, r20_all_en, dt_all))\n",
    "display(df_folds_en.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c65ed",
   "metadata": {},
   "source": [
    "[EN Fold 1] AUC=0.7263 | KS=0.3393 | P@20=0.1928 | R@20=0.4777 | 277.8s\n",
    "[EN Fold 2] AUC=0.7373 | KS=0.3500 | P@20=0.1946 | R@20=0.4822 | 5748.5s\n",
    "[EN Fold 3] AUC=0.7307 | KS=0.3414 | P@20=0.1938 | R@20=0.4802 | 412.5s\n",
    "[EN Fold 4] AUC=0.7368 | KS=0.3570 | P@20=0.1985 | R@20=0.4918 | 10313.8s\n",
    "[EN Fold 5] AUC=0.7283 | KS=0.3460 | P@20=0.1885 | R@20=0.4671 | 210.8s\n",
    "\n",
    "[EN OOF] AUC=0.7318 | KS=0.3456 | P@20=0.1937 | R@20=0.4799 | total=16963.9s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b6dcc",
   "metadata": {},
   "source": [
    "\n",
    "fold\tAUC\tKS\tP@20\tR@20\ttime_s\n",
    "0\t1\t0.7263\t0.3393\t0.1928\t0.4777\t277.7931\n",
    "1\t2\t0.7373\t0.3500\t0.1946\t0.4822\t5748.5468\n",
    "2\t3\t0.7307\t0.3414\t0.1938\t0.4802\t412.5458\n",
    "3\t4\t0.7368\t0.3570\t0.1985\t0.4918\t10313.8338\n",
    "4\t5\t0.7283\t0.3460\t0.1885\t0.4671\t210.7878\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee577b",
   "metadata": {},
   "source": [
    "### Elastic Net (Logistic, saga) — Cross-Validation Summary\n",
    "\n",
    "The elastic-net variant underperformed the L2 baseline and was extremely slow/variable to train.\n",
    "\n",
    "**Fold results (5-fold):**\n",
    "- AUC (mean): **0.732**  \n",
    "- KS (mean): **0.349**  \n",
    "- Precision@20% (mean): **0.194**  \n",
    "- Recall@20% (mean): **0.480**  \n",
    "- Training time per fold was highly unstable (from ~3–170 minutes), indicating poor scalability for this setup on our dataset (307k rows + OHE + target encoding).\n",
    "\n",
    "**Decision:** I will **drop the elastic-net logistic** as a baseline. The **L2 logistic (saga or lbfgs)** remains my reference model (OOF AUC ≈ **0.746**, KS ≈ **0.365**), and I will proceed to gradient boosting (LightGBM) for performance gains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bbe7e",
   "metadata": {},
   "source": [
    "# Persist transformed feature names\n",
    "\n",
    "For auditability and SHAP alignment later, I fit the preprocessor on the full data only to **extract feature names**.  \n",
    "This is for documentation only (I do not use this fit for evaluation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca347cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor once to extract feature names for documentation\n",
    "_ = preprocessor.fit(X, y)\n",
    "\n",
    "# Extract names safely\n",
    "# numeric: original names\n",
    "num_names = num_cols\n",
    "\n",
    "# one-hot names\n",
    "ohe = preprocessor.named_transformers_[\"ohe\"].named_steps[\"ohe\"]\n",
    "ohe_names = ohe.get_feature_names_out(low_card).tolist()\n",
    "\n",
    "# target-encoded: keep original high_card names (they become numeric columns)\n",
    "tgt_names = high_card\n",
    "\n",
    "feature_names = num_names + ohe_names + tgt_names\n",
    "\n",
    "pd.Series(feature_names, name=\"feature_name\").to_csv(\"artifacts/feature_names_logreg.csv\", index=False)\n",
    "print(f\"Saved {len(feature_names)} feature names to artifacts/feature_names_logreg.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6120cf",
   "metadata": {},
   "source": [
    "# Calibration diagnostics (Brier + reliability curve)\n",
    "\n",
    "Because I will interpret scores as PD estimates, I check probabilistic calibration using the Brier score and a reliability curve (predicted vs. observed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Use OOF probabilities from the chosen baseline (e.g., lbfgs patch)\n",
    "brier = brier_score_loss(y, oof)\n",
    "print(f\"Brier score (OOF): {brier:.4f}\")\n",
    "\n",
    "# Reliability curve\n",
    "prob_true, prob_pred = calibration_curve(y, oof, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed default rate\")\n",
    "plt.title(\"Reliability curve (OOF)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LightGBM + Optuna (CV) — sem Pipeline no fit, com eval_set transformado =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# >>> pressupõe X, y, preprocessor, ks_statistic, precision_at_k, recall_at_k já definidos <<<\n",
    "\n",
    "pos_rate = y.mean()\n",
    "scale_pos_weight = (1 - pos_rate) / pos_rate\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": 5000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 50, 500),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "    }\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    for tr, va in cv.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        # 1) Fit o preprocessor NO TREINO do fold e transforme treino/val\n",
    "        prep = clone(preprocessor)\n",
    "        prep.fit(X_tr, y_tr)\n",
    "        Xtr_t = prep.transform(X_tr)\n",
    "        Xva_t = prep.transform(X_va)\n",
    "\n",
    "        # 2) Modelo LGBM por fold\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "\n",
    "        # 3) LightGBM 4.x: early stopping via callbacks\n",
    "        clf.fit(\n",
    "            Xtr_t, y_tr,\n",
    "            eval_set=[(Xva_t, y_va)],\n",
    "            eval_metric=\"auc\",\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "                lgb.log_evaluation(period=0),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        oof[va] = clf.predict_proba(Xva_t)[:, 1]\n",
    "\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "\n",
    "# ===== OOF final com os melhores hiperparâmetros =====\n",
    "best = study.best_trial.params\n",
    "rows, oof_lgbm = [], np.zeros(len(X))\n",
    "\n",
    "for i, (tr, va) in enumerate(cv.split(X, y), 1):\n",
    "    X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    prep = clone(preprocessor)\n",
    "    prep.fit(X_tr, y_tr)\n",
    "    Xtr_t = prep.transform(X_tr)\n",
    "    Xva_t = prep.transform(X_va)\n",
    "\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        objective=\"binary\",\n",
    "        metric=\"auc\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        **best\n",
    "    )\n",
    "    clf.fit(\n",
    "        Xtr_t, y_tr,\n",
    "        eval_set=[(Xva_t, y_va)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "            lgb.log_evaluation(period=0),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    proba = clf.predict_proba(Xva_t)[:, 1]\n",
    "    oof_lgbm[va] = proba\n",
    "\n",
    "    auc = roc_auc_score(y_va, proba)\n",
    "    fpr, tpr, _ = roc_curve(y_va, proba)\n",
    "    ks  = np.max(tpr - fpr)\n",
    "    p20 = precision_at_k(y_va, proba, 0.2)\n",
    "    r20 = recall_at_k(y_va, proba, 0.2)\n",
    "    rows.append({\"fold\": i, \"AUC\": auc, \"KS\": ks, \"P@20\": p20, \"R@20\": r20})\n",
    "    print(f\"[LGBM Fold {i}] AUC={auc:.4f} | KS={ks:.4f} | P@20={p20:.4f} | R@20={r20:.4f}\")\n",
    "\n",
    "auc_all = roc_auc_score(y, oof_lgbm)\n",
    "fpr, tpr, _ = roc_curve(y, oof_lgbm)\n",
    "ks_all  = np.max(tpr - fpr)\n",
    "p20_all = precision_at_k(y, oof_lgbm, 0.2)\n",
    "r20_all = recall_at_k(y, oof_lgbm, 0.2)\n",
    "\n",
    "print(\"\\n[LGBM OOF] AUC={:.4f} | KS={:.4f} | P@20={:.4f} | R@20={:.4f}\"\n",
    "      .format(auc_all, ks_all, p20_all, r20_all))\n",
    "pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c642741",
   "metadata": {},
   "source": [
    "### LightGBM — Cross-Validation Results (OOF)\n",
    "\n",
    "LightGBM outperforms my logistic baseline across all decision metrics:\n",
    "\n",
    "- **AUC**: ~0.761 (vs 0.746)\n",
    "- **KS**: ~0.391 (vs 0.365)\n",
    "- **Precision@20%**: ~0.212 (vs 0.200)\n",
    "- **Recall@20%**: ~0.526 (vs 0.500)\n",
    "\n",
    "Takeaway: the boosted trees capture non-linearities and interactions que a regressão linear não alcança, mantendo robustez fora-da-amostra. I will proceed with **probability calibration** (if needed) and **model explainability (SHAP)** to communicate risk drivers to non-technical stakeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, warnings, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# --- utilities (reuse-safe) ---\n",
    "def ks_statistic(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float((tpr - fpr).max())\n",
    "\n",
    "def reliability_points(y_true, y_score, n_bins=10):\n",
    "    # quantile bins → predicted vs observed\n",
    "    qs = np.linspace(0, 1, n_bins + 1)\n",
    "    cuts = np.quantile(y_score, qs)\n",
    "    cuts[0], cuts[-1] = -np.inf, np.inf\n",
    "    bin_id = np.digitize(y_score, cuts[1:-1], right=True)\n",
    "    df = pd.DataFrame({\"bin\": bin_id, \"pred\": y_score, \"y\": y_true})\n",
    "    grp = df.groupby(\"bin\", as_index=False).agg(predicted=(\"pred\", \"mean\"),\n",
    "                                                observed=(\"y\", \"mean\"),\n",
    "                                                count=(\"y\",\"size\"))\n",
    "    return grp\n",
    "\n",
    "# --- ensure artifacts dir ---\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# --- grab best params from Optuna if present; else fallback ---\n",
    "try:\n",
    "    best  # type: ignore\n",
    "except NameError:\n",
    "    best = {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 127,\n",
    "        \"min_data_in_leaf\": 100,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 3,\n",
    "        \"max_depth\": -1,\n",
    "        \"lambda_l1\": 0.0,\n",
    "        \"lambda_l2\": 2.0,\n",
    "    }\n",
    "\n",
    "# --- build final LGBM with best params ---\n",
    "pos_rate = y.mean()\n",
    "scale_pos_weight = (1 - pos_rate) / pos_rate\n",
    "base_lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=5000,\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    **best\n",
    ")\n",
    "\n",
    "# ------------ FAST MODE (single holdout) ------------\n",
    "STRICT_CV = False   # << troque para True se quiser calibração por fold (lenta)\n",
    "\n",
    "if not STRICT_CV:\n",
    "    print(\">> FAST calibration mode (single stratified holdout = 10%)\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) split for calibration (10%)\n",
    "    X_tr, X_cal, y_tr, y_cal = train_test_split(\n",
    "        X, y, test_size=0.10, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 2) fit preprocessor on training-only; transform train & cal\n",
    "    prep = clone(preprocessor)\n",
    "    prep.fit(X_tr, y_tr)\n",
    "    Xtr_t = prep.transform(X_tr)\n",
    "    Xcal_t = prep.transform(X_cal)\n",
    "\n",
    "    # 3) train LightGBM with early stopping using the calibration split for validation\n",
    "    base_lgbm.fit(\n",
    "        Xtr_t, y_tr,\n",
    "        eval_set=[(Xcal_t, y_cal)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    # 4) raw probabilities on calibration split (to learn isotonic)\n",
    "    proba_cal_raw = base_lgbm.predict_proba(Xcal_t)[:, 1]\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(proba_cal_raw, y_cal)\n",
    "\n",
    "    # 5) OOF-like evaluation by re-scoring full data (transform once)\n",
    "    Xall_t = prep.transform(X)\n",
    "    proba_raw_all = base_lgbm.predict_proba(Xall_t)[:, 1]\n",
    "    proba_cal_all = iso.transform(proba_raw_all)\n",
    "\n",
    "    # 6) metrics\n",
    "    auc_raw  = roc_auc_score(y, proba_raw_all)\n",
    "    auc_cal  = roc_auc_score(y, proba_cal_all)\n",
    "    ks_raw   = ks_statistic(y, proba_raw_all)\n",
    "    ks_cal   = ks_statistic(y, proba_cal_all)\n",
    "    brier_raw = brier_score_loss(y, proba_raw_all)\n",
    "    brier_cal = brier_score_loss(y, proba_cal_all)\n",
    "    print(f\"[FAST] Raw  AUC={auc_raw:.4f} | KS={ks_raw:.4f} | Brier={brier_raw:.4f}\")\n",
    "    print(f\"[FAST] Cal  AUC={auc_cal:.4f} | KS={ks_cal:.4f} | Brier={brier_cal:.4f}\")\n",
    "    print(f\"Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # 7) reliability points (for plot downstream if you want)\n",
    "    rel_raw = reliability_points(y.values, proba_raw_all, n_bins=10)\n",
    "    rel_cal = reliability_points(y.values, proba_cal_all, n_bins=10)\n",
    "\n",
    "else:\n",
    "    # ------------ STRICT CV MODE (slower, leakage-safe per fold) ------------\n",
    "    print(\">> STRICT CV calibration mode (isotonic per fold) — SLOW\")\n",
    "    t0 = time.time()\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_raw = np.zeros(len(X), dtype=float)\n",
    "    oof_cal = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(cv.split(X, y), 1):\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        # inner split for isotonic train (10% of training fold)\n",
    "        inner = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=fold)\n",
    "        fit_idx, cal_idx = next(inner.split(X_tr, y_tr))\n",
    "        X_fit, y_fit = X_tr.iloc[fit_idx], y_tr.iloc[fit_idx]\n",
    "        X_cal, y_cal = X_tr.iloc[cal_idx], y_tr.iloc[cal_idx]\n",
    "\n",
    "        # fit preprocessor and transform\n",
    "        prep = clone(preprocessor)\n",
    "        prep.fit(X_fit, y_fit)\n",
    "        Xfit_t = prep.transform(X_fit)\n",
    "        Xcal_t = prep.transform(X_cal)\n",
    "        Xva_t  = prep.transform(X_va)\n",
    "\n",
    "        # train model\n",
    "        clf = clone(base_lgbm)\n",
    "        clf.fit(\n",
    "            Xfit_t, y_fit,\n",
    "            eval_set=[(Xva_t, y_va)],\n",
    "            eval_metric=\"auc\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "                       lgb.log_evaluation(period=0)]\n",
    "        )\n",
    "\n",
    "        # raw proba on val fold\n",
    "        proba_raw = clf.predict_proba(Xva_t)[:, 1]\n",
    "        oof_raw[va] = proba_raw\n",
    "\n",
    "        # isotonic fitted on calibration split of training fold\n",
    "        proba_cal_train = clf.predict_proba(Xcal_t)[:, 1]\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        # (speed tip) subsample calibration if huge:\n",
    "        if len(proba_cal_train) > 50000:\n",
    "            idx = np.random.RandomState(42).choice(len(proba_cal_train), 50000, replace=False)\n",
    "            iso.fit(proba_cal_train[idx], y_cal.iloc[idx])\n",
    "        else:\n",
    "            iso.fit(proba_cal_train, y_cal)\n",
    "\n",
    "        oof_cal[va] = iso.transform(proba_raw)\n",
    "\n",
    "    # aggregate metrics\n",
    "    auc_raw  = roc_auc_score(y, oof_raw)\n",
    "    auc_cal  = roc_auc_score(y, oof_cal)\n",
    "    ks_raw   = ks_statistic(y, oof_raw)\n",
    "    ks_cal   = ks_statistic(y, oof_cal)\n",
    "    brier_raw = brier_score_loss(y, oof_raw)\n",
    "    brier_cal = brier_score_loss(y, oof_cal)\n",
    "    print(f\"[CV] Raw  AUC={auc_raw:.4f} | KS={ks_raw:.4f} | Brier={brier_raw:.4f}\")\n",
    "    print(f\"[CV] Cal  AUC={auc_cal:.4f} | KS={ks_cal:.4f} | Brier={brier_cal:.4f}\")\n",
    "    print(f\"Done in {time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be6be8",
   "metadata": {},
   "source": [
    ">> FAST calibration mode (single stratified holdout = 10%)\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[LightGBM] [Info] Number of positive: 22342, number of negative: 254417\n",
    "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033898 seconds.\n",
    "You can set `force_row_wise=true` to remove the overhead.\n",
    "And if memory is not enough, you can set `force_col_wise=true`.\n",
    "[LightGBM] [Info] Total Bins 11392\n",
    "[LightGBM] [Info] Number of data points in the train set: 276759, number of used features: 151\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "...\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
    "  warnings.warn(\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
    "  warnings.warn(\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[FAST] Raw  AUC=0.8355 | KS=0.5143 | Brier=0.1709\n",
    "[FAST] Cal  AUC=0.8352 | KS=0.5141 | Brier=0.0639\n",
    "Done in 49.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ccc6d",
   "metadata": {},
   "source": [
    "## B) SHAP — Global & Local (fast and production-friendly)\n",
    "\n",
    "I refit the final model on a train/holdout split (10%) with early stopping, then:\n",
    "- Use **TreeExplainer** and a **capped sample** (e.g., 8k) for global plots (fast).\n",
    "- Show a **local** explanation (top-risk case from the holdout).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0191ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHAP — Global & Local (robusto e rápido) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Helper para garantir 2D e denso\n",
    "def ensure_2d_dense(X):\n",
    "    try:\n",
    "        from scipy import sparse\n",
    "        if sparse.issparse(X):\n",
    "            X = X.toarray()\n",
    "    except Exception:\n",
    "        pass\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    return X\n",
    "\n",
    "# 1) Split para SHAP (rápido e consistente)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y, test_size=0.10, stratify=y, random_state=123\n",
    ")\n",
    "\n",
    "prep_full = clone(preprocessor)\n",
    "prep_full.fit(X_tr, y_tr)\n",
    "Xtr_t = prep_full.transform(X_tr)\n",
    "Xva_t = prep_full.transform(X_va)\n",
    "\n",
    "Xtr_t = ensure_2d_dense(Xtr_t)\n",
    "Xva_t = ensure_2d_dense(Xva_t)\n",
    "\n",
    "# 2) Refit do LGBM (usa seus 'best' params; se não houver, cai no fallback)\n",
    "try:\n",
    "    best\n",
    "except NameError:\n",
    "    best = {\n",
    "        \"learning_rate\": 0.05, \"num_leaves\": 127, \"min_data_in_leaf\": 100,\n",
    "        \"feature_fraction\": 0.9, \"bagging_fraction\": 0.8, \"bagging_freq\": 3,\n",
    "        \"max_depth\": -1, \"lambda_l1\": 0.0, \"lambda_l2\": 2.0\n",
    "    }\n",
    "\n",
    "pos_rate = y.mean()\n",
    "final_lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=5000, objective=\"binary\", metric=\"auc\",\n",
    "    n_jobs=-1, random_state=123, scale_pos_weight=(1-pos_rate)/pos_rate,\n",
    "    **best\n",
    ")\n",
    "final_lgbm.fit(\n",
    "    Xtr_t, y_tr,\n",
    "    eval_set=[(Xva_t, y_va)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "        lgb.log_evaluation(period=0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 3) Nomes de features alinhados ao transform\n",
    "try:\n",
    "    feat_names = pd.read_csv(\"artifacts/feature_names_logreg.csv\")[\"feature_name\"].tolist()\n",
    "except Exception:\n",
    "    num_cols  = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    cat_cols  = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    low_card  = [c for c in cat_cols if X[c].nunique() < 10]\n",
    "    high_card = [c for c in cat_cols if X[c].nunique() >= 10]\n",
    "    ohe = prep_full.named_transformers_[\"ohe\"].named_steps[\"ohe\"]\n",
    "    ohe_names = ohe.get_feature_names_out(low_card).tolist()\n",
    "    feat_names = [f\"num__{c}\" for c in num_cols] + ohe_names + [f\"tgt__{c}\" for c in high_card]\n",
    "\n",
    "# 4) SHAP Global (amostra controlada + classe positiva)\n",
    "explainer = shap.TreeExplainer(final_lgbm)\n",
    "N_SHAP = min(8000, Xtr_t.shape[0])\n",
    "rng = np.random.RandomState(42)\n",
    "idx = rng.choice(Xtr_t.shape[0], N_SHAP, replace=False)\n",
    "X_global = Xtr_t[idx]\n",
    "\n",
    "# Algumas versões retornam lista [class0, class1]; outras retornam ndarray direto\n",
    "shap_vals = explainer.shap_values(X_global, check_additivity=False)\n",
    "if isinstance(shap_vals, list):\n",
    "    shap_pos = shap_vals[1]\n",
    "else:\n",
    "    shap_pos = shap_vals\n",
    "\n",
    "# Plots globais\n",
    "shap.summary_plot(shap_pos, X_global, feature_names=feat_names[:X_global.shape[1]], plot_type=\"bar\", show=True)\n",
    "shap.summary_plot(shap_pos, X_global, feature_names=feat_names[:X_global.shape[1]], show=True)\n",
    "\n",
    "# 5) SHAP Local (cliente mais arriscado do holdout)\n",
    "proba_holdout = final_lgbm.predict_proba(Xva_t)[:, 1]\n",
    "i = int(np.argsort(-proba_holdout)[0])\n",
    "\n",
    "x_row = Xva_t[i:i+1]  # <- fatia 2D\n",
    "shap_row = explainer.shap_values(x_row, check_additivity=False)\n",
    "if isinstance(shap_row, list):\n",
    "    shap_row = shap_row[1]   # classe positiva\n",
    "shap_row = shap_row[0]       # (1, n_features) -> (n_features,)\n",
    "\n",
    "base_val = explainer.expected_value\n",
    "if isinstance(base_val, (list, np.ndarray)):\n",
    "    base_val = base_val[1]\n",
    "\n",
    "ex_local = shap.Explanation(\n",
    "    values=shap_row,\n",
    "    base_values=base_val,\n",
    "    data=x_row[0],\n",
    "    feature_names=feat_names[:x_row.shape[1]],\n",
    ")\n",
    "shap.plots.waterfall(ex_local, max_display=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b55b21",
   "metadata": {},
   "source": [
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[LightGBM] [Info] Number of positive: 22342, number of negative: 254417\n",
    "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027232 seconds.\n",
    "You can set `force_row_wise=true` to remove the overhead.\n",
    "And if memory is not enough, you can set `force_col_wise=true`.\n",
    "[LightGBM] [Info] Total Bins 11366\n",
    "[LightGBM] [Info] Number of data points in the train set: 276759, number of used features: 151\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080727 -> initscore=-2.432506\n",
    "...\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd8b15",
   "metadata": {},
   "source": [
    "## C) Persist artifacts & provide `score_applications(df)`\n",
    "\n",
    "I persist the **preprocessor**, **LightGBM model**, **isotonic calibrator** and **feature names**, and expose a `score_applications(df)` function that returns **calibrated PD** ready for dashboards/APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6509397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# --- choose which fitted objects to persist ---\n",
    "# From FAST calibration block:\n",
    "#   - 'prep'  : fitted on full training (or training part)\n",
    "#   - 'base_lgbm': fitted LGBM (trained with early stopping)\n",
    "#   - 'iso'   : isotonic calibrator fitted on holdout\n",
    "# If you ran STRICT_CV instead, you may prefer to refit a single model like below before saving.\n",
    "\n",
    "# If FAST mode variables don't exist (e.g., you executed STRICT_CV only), refit quickly here:\n",
    "try:\n",
    "    prep, base_lgbm, iso  # type: ignore\n",
    "except NameError:\n",
    "    # quick refit on single holdout to produce production artifacts\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.base import clone\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    X_tr, X_cal, y_tr, y_cal = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)\n",
    "    prep = clone(preprocessor).fit(X_tr, y_tr)\n",
    "    Xtr_t = prep.transform(X_tr)\n",
    "    Xcal_t = prep.transform(X_cal)\n",
    "\n",
    "    base_lgbm = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        objective=\"binary\",\n",
    "        metric=\"auc\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=(1 - y.mean())/y.mean(),\n",
    "        **best\n",
    "    ).fit(\n",
    "        Xtr_t, y_tr,\n",
    "        eval_set=[(Xcal_t, y_cal)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "    # isotonic on calibration set\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\").fit(\n",
    "        base_lgbm.predict_proba(Xcal_t)[:,1], y_cal\n",
    "    )\n",
    "\n",
    "# --- persist artifacts ---\n",
    "joblib.dump(prep,      \"artifacts/preprocessor.joblib\")\n",
    "joblib.dump(base_lgbm, \"artifacts/lgbm_model.joblib\")\n",
    "joblib.dump(iso,       \"artifacts/isotonic.joblib\")\n",
    "\n",
    "# feature names (if you have them)\n",
    "try:\n",
    "    pd.read_csv(\"artifacts/feature_names_logreg.csv\")  # ensure exists\n",
    "except Exception:\n",
    "    # build and save current names\n",
    "    num_cols  = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    cat_cols  = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    low_card  = [c for c in cat_cols if X[c].nunique() < 10]\n",
    "    high_card = [c for c in cat_cols if X[c].nunique() >= 10]\n",
    "    ohe = prep.named_transformers_[\"ohe\"].named_steps[\"ohe\"]\n",
    "    ohe_names = ohe.get_feature_names_out(low_card).tolist()\n",
    "    feat_names = [f\"num__{c}\" for c in num_cols] + ohe_names + [f\"tgt__{c}\" for c in high_card]\n",
    "    pd.Series(feat_names, name=\"feature_name\").to_csv(\"artifacts/feature_names_logreg.csv\", index=False)\n",
    "\n",
    "# best params\n",
    "with open(\"artifacts/best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(\"Saved: preprocessor.joblib, lgbm_model.joblib, isotonic.joblib, feature_names_logreg.csv, best_params.json\")\n",
    "\n",
    "# --- production scorer ---\n",
    "def score_applications(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return calibrated PD for new applications DataFrame.\n",
    "    Expects same columns as training raw schema (including categoricals).\n",
    "    \"\"\"\n",
    "    prep = joblib.load(\"artifacts/preprocessor.joblib\")\n",
    "    model = joblib.load(\"artifacts/lgbm_model.joblib\")\n",
    "    iso   = joblib.load(\"artifacts/isotonic.joblib\")\n",
    "\n",
    "    Xt = prep.transform(df)\n",
    "    proba_raw = model.predict_proba(Xt)[:, 1]\n",
    "    pd_cal = iso.transform(proba_raw)\n",
    "    return pd.DataFrame({\n",
    "        \"SK_ID_CURR\": df.get(\"SK_ID_CURR\", pd.Series(range(len(df)))),\n",
    "        \"pd_raw\": proba_raw,\n",
    "        \"pd_calibrated\": pd_cal\n",
    "    })\n",
    "\n",
    "# --- quick smoke test on a small slice of training ---\n",
    "sample = X.head(5).copy()\n",
    "out = score_applications(sample)\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e2508",
   "metadata": {},
   "source": [
    "Saved: preprocessor.joblib, lgbm_model.joblib, isotonic.joblib, feature_names_logreg.csv, best_params.json\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e369e91",
   "metadata": {},
   "source": [
    "\n",
    "SK_ID_CURR\tpd_raw\tpd_calibrated\n",
    "0\t0\t0.910207\t0.555556\n",
    "1\t1\t0.272722\t0.040662\n",
    "2\t2\t0.235538\t0.030796\n",
    "3\t3\t0.335316\t0.045103\n",
    "4\t4\t0.546277\t0.108856\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094743c",
   "metadata": {},
   "source": [
    "### Probability Calibration — Reliability Curve (raw vs calibrated)\n",
    "\n",
    "I plot calibration curves on the full training set using the fitted preprocessor, model and isotonic calibrator.  \n",
    "Lower Brier and curves closer to the diagonal confirm better PD quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import joblib\n",
    "\n",
    "# Load persisted artifacts\n",
    "prep  = joblib.load(\"artifacts/preprocessor.joblib\")\n",
    "model = joblib.load(\"artifacts/lgbm_model.joblib\")\n",
    "iso   = joblib.load(\"artifacts/isotonic.joblib\")\n",
    "\n",
    "# Transform and score (full training)\n",
    "Xt = prep.transform(X)\n",
    "pd_raw = model.predict_proba(Xt)[:, 1]\n",
    "pd_cal = iso.transform(pd_raw)\n",
    "\n",
    "# Brier\n",
    "print(f\"Brier raw={brier_score_loss(y, pd_raw):.4f} | Brier cal={brier_score_loss(y, pd_cal):.4f}\")\n",
    "\n",
    "# Reliability (quantile bins)\n",
    "pt_raw, pp_raw = calibration_curve(y, pd_raw, n_bins=10, strategy=\"quantile\")\n",
    "pt_cal, pp_cal = calibration_curve(y, pd_cal, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([0,1],[0,1],\"--\",linewidth=1)\n",
    "plt.plot(pp_raw, pt_raw, marker=\"o\", label=\"Raw\")\n",
    "plt.plot(pp_cal, pt_cal, marker=\"o\", label=\"Calibrated (Isotonic)\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed default rate\")\n",
    "plt.title(\"Reliability Curve — OOF-style (raw vs calibrated)\")\n",
    "plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4461f",
   "metadata": {},
   "source": [
    "c:\\Users\\luizo\\anaconda3\\envs\\credit-risk\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
    "  warnings.warn(\n",
    "[LightGBM] [Warning] min_data_in_leaf is set=466, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=466\n",
    "[LightGBM] [Warning] feature_fraction is set=0.823409610342382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.823409610342382\n",
    "[LightGBM] [Warning] lambda_l1 is set=2.510486193166801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.510486193166801\n",
    "[LightGBM] [Warning] lambda_l2 is set=5.670894047918985, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.670894047918985\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9377595784067674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9377595784067674\n",
    "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
    "Brier raw=0.1709 | Brier cal=0.0639"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98618",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
